{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexSplitPorter(doc):\n",
    "    termes = doc.split()\n",
    "    MotsVides = nltk.corpus.stopwords.words('english')\n",
    "    TermesSansMotsVides = [terme for terme in termes if terme.lower() not in MotsVides]\n",
    "    Porter = nltk.PorterStemmer()\n",
    "    TermesNormalisation = [Porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = Counter(TermesNormalisation)\n",
    "    max_frequency = max(TermesFrequence.values())\n",
    "    TermesPoids = [(terme, frequence) for terme, frequence in TermesFrequence.items()]\n",
    "    return TermesPoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexSplitLancester(doc):\n",
    "    termes = doc.split()\n",
    "    MotsVides = nltk.corpus.stopwords.words('english')\n",
    "    TermesSansMotsVides = [terme for terme in termes if terme.lower() not in MotsVides]\n",
    "    Lancaster = nltk.LancasterStemmer()\n",
    "    TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = Counter(TermesNormalisation)\n",
    "    max_frequency = max(TermesFrequence.values())\n",
    "    TermesPoids = [(terme, frequence) for terme, frequence in TermesFrequence.items()]\n",
    "    return TermesPoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexTokenPorter(doc):\n",
    "    ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*')\n",
    "    termes = ExpReg.tokenize(doc)\n",
    "    MotsVides = nltk.corpus.stopwords.words('english')\n",
    "    TermesSansMotsVides = [terme for terme in termes if terme.lower() not in MotsVides]\n",
    "    Porter = nltk.PorterStemmer()\n",
    "    TermesNormalisation = [Porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = Counter(TermesNormalisation)\n",
    "    max_frequency = max(TermesFrequence.values())\n",
    "    TermesPoids = [(terme, frequence) for terme, frequence in TermesFrequence.items()]\n",
    "    return TermesPoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexTokenLancester(doc):\n",
    "    ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*')\n",
    "    termes = ExpReg.tokenize(doc)\n",
    "    MotsVides = nltk.corpus.stopwords.words('english')\n",
    "    TermesSansMotsVides = [terme for terme in termes if terme.lower() not in MotsVides]\n",
    "    Lancaster = nltk.LancasterStemmer()\n",
    "    TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "    TermesFrequence = Counter(TermesNormalisation)\n",
    "    max_frequency = max(TermesFrequence.values())\n",
    "    TermesPoids = [(terme, frequence) for terme, frequence in TermesFrequence.items()]\n",
    "    return TermesPoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDescriptionFile(numdoc,doc,methode):\n",
    "    output_file = f\"Resultats/Descripteur{methode}.txt\"\n",
    "    method_function = globals().get(methode)\n",
    "    if method_function:\n",
    "        Terms = method_function(doc)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method name\")\n",
    "    with open(output_file, 'a') as output:\n",
    "        for term in Terms:\n",
    "            term_text = term[0]  \n",
    "            term_frequency = term[1]\n",
    "            output.write(f'{numdoc} {term_text} {term_frequency}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInverseFile(numdoc,doc,methode):\n",
    "    output_file = f\"Resultats/Inverse{methode}.txt\"\n",
    "    method_function = globals().get(methode)\n",
    "    if method_function:\n",
    "        Terms = method_function(doc)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method name\")\n",
    "    with open(output_file, 'a') as output:\n",
    "        for term in Terms:\n",
    "            term_text = term[0]  \n",
    "            term_frequency = term[1]\n",
    "            output.write(f'{term_text} {term_frequency} {numdoc} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_document_content(path):\n",
    "        doc = pd.read_csv(path, sep=',', skipinitialspace=True) \n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methodes = [\"indexSplitPorter\",\"indexSplitLancester\",\"indexTokenPorter\",\"indexTokenLancester\"]\n",
    "documents = get_document_content(\"./Collection/documents.csv\")\n",
    "N = len(documents)\n",
    "for i  in range(0, N):\n",
    "    document_content = str(documents.iloc[i]['title']) + ' ' + str(documents.iloc[i]['text'])\n",
    "    num = documents.iloc[i]['doc_num']\n",
    "    for methode in Methodes:\n",
    "        createDescriptionFile(num,document_content,methode)\n",
    "        createInverseFile(num,document_content,methode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def createIndexesPondere(index_file, num_docs):\n",
    "    with open(index_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified_lines = []\n",
    "\n",
    "    term_frequency = defaultdict(int)\n",
    "    document_frequency = defaultdict(int)\n",
    "    max_term_frequency = 0  \n",
    "\n",
    "    term_frequency = defaultdict(int)\n",
    "    document_frequency = defaultdict(int)\n",
    "    max_term_frequency = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            numdoc, term, term_freq = parts[:3]\n",
    "            numdoc = int(numdoc)\n",
    "            term_freq = int(term_freq)\n",
    "            term_frequency[(numdoc, term)] += term_freq\n",
    "            document_frequency[term] += 1\n",
    "            max_term_frequency[numdoc] = max(max_term_frequency.get(numdoc, 0), term_frequency[(numdoc, term)])\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            numdoc, term, term_freq = parts[:3]\n",
    "            numdoc = int(numdoc)\n",
    "            term_freq = int(term_freq)\n",
    "\n",
    "            max_freq_in_doc = max_term_frequency.get(numdoc, 1) \n",
    "            weight = (term_freq / max_freq_in_doc) * math.log10(num_docs / document_frequency[term] + 1)\n",
    "\n",
    "            formatted_weight = f\"{weight:.4f}\"\n",
    "            modified_line = f\"{numdoc} {term} {term_freq} {formatted_weight}\\n\"\n",
    "            modified_lines.append(modified_line)\n",
    "\n",
    "    with open(index_file, 'w') as output_file:\n",
    "        output_file.writelines(modified_lines)\n",
    "def createInversePondere(index_file, num_docs):\n",
    "    with open(index_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified_lines = []\n",
    "\n",
    "    term_frequency = defaultdict(int)\n",
    "    document_frequency = defaultdict(int)\n",
    "    max_term_frequency = 0  \n",
    "\n",
    "    term_frequency = defaultdict(int)\n",
    "    document_frequency = defaultdict(int)\n",
    "    max_term_frequency = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            term, term_freq, numdoc = parts[:3]\n",
    "            numdoc = int(numdoc)\n",
    "            term_freq = int(term_freq)\n",
    "            term_frequency[(numdoc, term)] += term_freq\n",
    "            document_frequency[term] += 1\n",
    "            max_term_frequency[numdoc] = max(max_term_frequency.get(numdoc, 0), term_frequency[(numdoc, term)])\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            term, term_freq, numdoc = parts[:3]\n",
    "            numdoc = int(numdoc)\n",
    "            term_freq = int(term_freq)\n",
    "\n",
    "            max_freq_in_doc = max_term_frequency.get(numdoc, 1)  # Default to 1 if the document has no terms\n",
    "            weight = (term_freq / max_freq_in_doc) * math.log10(num_docs / document_frequency[term] + 1)\n",
    "\n",
    "            formatted_weight = f\"{weight:.4f}\"\n",
    "            modified_line = f\"{term} {term_freq} {numdoc} {formatted_weight}\\n\"\n",
    "            modified_lines.append(modified_line)\n",
    "\n",
    "    with open(index_file, 'w') as output_file:\n",
    "        output_file.writelines(modified_lines)\n",
    "\n",
    "index_files = ['Resultats/DescripteurindexSplitLancester.txt','Resultats/DescripteurindexSplitPorter.txt','Resultats/DescripteurindexTokenLancester.txt','Resultats/DescripteurindexTokenPorter.txt']\n",
    "for index in index_files:\n",
    "    createIndexesPondere(index,5999)\n",
    "\n",
    "inverse_files = ['Resultats/InverseindexSplitLancester.txt','Resultats/InverseindexSplitPorter.txt','Resultats/InverseindexTokenLancester.txt','Resultats/InverseindexTokenPorter.txt']\n",
    "for index in inverse_files:\n",
    "    createInversePondere(index,5999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
